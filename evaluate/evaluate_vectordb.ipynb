{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUEKFXIAdhlo",
        "outputId": "9e64d9ae-6ced-4867-c58b-ababc753df92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate==0.31.0\n",
        "!pip install annoy\n",
        "!pip install jsonlines\n",
        "!pip install transformers\n",
        "!pip install auto-gptq\n",
        "!pip install --upgrade accelerate\n",
        "!pip install langchain_cohere\n",
        "!pip install langchain_openai\n",
        "!pip install paddlepaddle\n",
        "!pip install langchain_chroma"
      ],
      "metadata": {
        "id": "WsQUtYLIdjP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from embeddings import CustomEmbeddings\n",
        "import jsonlines\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from vectorstores import VectorDatabase\n",
        "\n",
        "metamorphic = ['word_swap', 'obj_sub', 'verb_sub', 'nega_exp', 'word_del', 'num_sub', 'err_translate', 'err_nli']\n",
        "distance_metrics = ['cosine', 'euclidean', 'person', 'manhattan', 'lancewilliams', 'mahalanobis', 'braycurtis']\n",
        "\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_json(path, lines=True)\n",
        "    return df[['sentence1', 'sentence2', 'sentence3']].values.tolist()\n",
        "\n",
        "\n",
        "vector_dbs = ['Annoy', 'ScanNN', 'Chroma']\n",
        "distance_metric = None\n",
        "\n",
        "embedding = CustomEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", subsets=metamorphic)\n"
      ],
      "metadata": {
        "id": "zZ_vzY219HXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_embeddings = CustomEmbeddings(model_name=\"ada002\", subsets=metamorphic)"
      ],
      "metadata": {
        "id": "__RQMreGHyS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_all = []\n",
        "for vector_db in ['Annoy']:\n",
        "  for data_type in ['normal', 'variant']:\n",
        "    all_dataset = []\n",
        "    for me in metamorphic:\n",
        "        dataset = load_dataset('data/MeTMaP/dataset/'+ data_type +'/'+me+'.jsonl')\n",
        "        all_dataset.append(dataset)\n",
        "\n",
        "    acc = 0\n",
        "    for dataset in all_dataset:\n",
        "        for b, p, n in dataset:\n",
        "          # print(vector_db)\n",
        "          vb = VectorDatabase([p, n],\n",
        "                        embedding, vector_db)\n",
        "          candidates = vb.simulate_retrieval(b)\n",
        "          for c in candidates:\n",
        "            if c.page_content == p:\n",
        "              acc += 1\n",
        "              break\n",
        "            elif c.page_content == n:\n",
        "              break\n",
        "\n",
        "    acc /= len(all_dataset) * 5000\n",
        "    acc_all.append(acc)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t_jbPF6Jd3ze",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW6TEBqmpjlg",
        "outputId": "28e847f9-ed03-4ea1-864b-6318003572a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.377175, 0.997175]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_embeddings = CustomEmbeddings(model_name=\"sentence-transformers_all-MiniLM-L6-v2\", subsets=metamorphic)"
      ],
      "metadata": {
        "id": "w1ohHKUEqx4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_all = []\n",
        "for vector_db in ['Chroma']:\n",
        "  for data_type in ['normal', 'variant']:\n",
        "    all_dataset = []\n",
        "    for me in metamorphic:\n",
        "        dataset = load_dataset('data/MeTMaP/dataset/'+ data_type +'/'+me+'.jsonl')\n",
        "        all_dataset.append(dataset)\n",
        "\n",
        "    acc = 0\n",
        "    for dataset in all_dataset:\n",
        "        for b, p, n in dataset:\n",
        "          vb = VectorDatabase([p, n],\n",
        "                        chroma_embeddings, vector_db)\n",
        "          candidates = vb.simulate_retrieval(b)\n",
        "          for c in candidates:\n",
        "            if c.page_content == p:\n",
        "              acc += 1\n",
        "              break\n",
        "            elif c.page_content == n:\n",
        "              break\n",
        "        print(acc)\n",
        "\n",
        "    acc /= len(all_dataset) * 5000\n",
        "    acc_all.append(acc)"
      ],
      "metadata": {
        "id": "MNfN5qu7GBPN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}